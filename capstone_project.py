# -*- coding: utf-8 -*-
"""Capstone Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7paK1e7ZFQ2jb3OeCjXi7TVigZm8MhI
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/CAR DETAILS.csv')

df.head()

df.shape

df.dtypes

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

df['year'].value_counts()

df['year'].nunique()

df['year'].unique()

df['km_driven'].value_counts()

df['km_driven'].nunique()

df['fuel'].value_counts()

df['seller_type'].value_counts()

df['transmission'].value_counts()

df['owner'].value_counts()

df.describe()

plt.figure(figsize=(12, 6))
sns.histplot(df['selling_price'], kde=True, bins=30, color='blue')
plt.title('Distribution of Selling Prices')
plt.xlabel('Selling Price')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(df['km_driven'], kde=True, bins=30, color='green')
plt.title('Distribution of Kilometers Driven')
plt.xlabel('Kilometers Driven')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 5))
sns.countplot(data=df, x='fuel', palette='Set2')
plt.title('Fuel Type Distribution')
plt.show()

plt.figure(figsize=(10, 5))
sns.countplot(data=df, x='transmission', palette='Set3')
plt.title('Transmission Type Distribution')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='fuel', y='selling_price', data=df)
plt.title('Selling Price vs Fuel Type')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='transmission', y='selling_price', data=df)
plt.title('Selling Price vs Transmission Type')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='km_driven', y='selling_price', hue='fuel', data=df)
plt.title('Selling Price vs Kilometers Driven')
plt.show()

df.drop('name', axis=1, inplace=True)

# 1. Identify the categorical columns to apply one-hot encoding
categorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']

# 2. Apply one-hot encoding using pandas `get_dummies()`
df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)
df_encoded = df_encoded.astype(int)
# 3. Display the first few rows of the encoded dataframe
df_encoded.head()

df_encoded.dtypes

plt.figure(figsize=(12, 8))
sns.heatmap(df_encoded.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df_encoded)

df_scaled = pd.DataFrame(df_scaled, columns=df_encoded.columns)

df_scaled.head()

x = df_scaled.drop('selling_price', axis=1)
y = df_scaled['selling_price']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

x_train.head()

from sklearn.linear_model import LinearRegression,Ridge,Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def reg_eval_metrics(y, ypred):
    mae = mean_absolute_error(y, ypred)
    mse = mean_squared_error(y, ypred)
    rmse = np.sqrt(mean_squared_error(y, ypred))
    r2 = r2_score(y, ypred)
    print("MAE:", mae)
    print("MSE:", mse)
    print("RMSE:", rmse)
    print("R2 Score:", r2)

def train_test_scr(model):
    print('Training Score',model.score(x_train,y_train))
    print('Testing Score',model.score(x_test,y_test))

lr = LinearRegression()
lr.fit(x_train, y_train)

ypred_lr = lr.predict(x_test)

train_test_scr(lr)

reg_eval_metrics(y_test, ypred_lr)

l1 = Lasso()
l1.fit(x_train, y_train)

ypred_l1 = l1.predict(x_test)

train_test_scr(l1)

reg_eval_metrics(y_test, ypred_l1)

l2 = Ridge()
l2.fit(x_train, y_train)

ypred_l2 = l2.predict(x_test)

train_test_scr(l2)

reg_eval_metrics(y_test, ypred_l2)

from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import StackingRegressor

knn = KNeighborsRegressor()
knn.fit(x_train, y_train)

ypred_knn = knn.predict(x_test)

train_test_scr(knn)

reg_eval_metrics(y_test, ypred_knn)

dt = DecisionTreeRegressor()
dt.fit(x_train, y_train)

ypred_dt = dt.predict(x_test)

train_test_scr(dt)

reg_eval_metrics(y_test, ypred_dt)

rf = RandomForestRegressor()
rf.fit(x_train, y_train)

ypred_rf = rf.predict(x_test)

train_test_scr(rf)

reg_eval_metrics(y_test, ypred_rf)

bag = BaggingRegressor()
bag.fit(x_train, y_train)

ypred_bag = bag.predict(x_test)

train_test_scr(bag)

reg_eval_metrics(y_test, ypred_bag)

import pickle

pickle.dump(lr, open('lr_model.pkl', 'wb'))

loaded_model = pickle.load(open('lr_model.pkl', 'rb'))

df1 = df.sample(n=20,random_state=7)

df1_x = df1.drop('selling_price', axis=1)
df1_y = df1['selling_price']
df1_x.shape, df1_y.shape

# 1. Identify the categorical columns to apply one-hot encoding
categorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']

# 2. Apply one-hot encoding using pandas `get_dummies()`
df1_x_encoded = pd.get_dummies(df1_x, columns=categorical_columns, drop_first=True)
df1_x_encoded = df1_x_encoded.astype(int)
# 3. Display the first few rows of the encoded dataframe
df1_x_encoded.head()
df1_x_encoded.shape

df1_x_scaled = scaler.fit_transform(df1_x_encoded)

df1_x_scaled = pd.DataFrame(df1_x_scaled, columns=df1_x_encoded.columns)

df1_x_scaled.head()

x1_train, x1_test, y1_train, y1_test = train_test_split(df1_x_scaled, df1_y, test_size=0.2, random_state=42)

x1_train.shape, x1_test.shape, y1_train.shape, y1_test.shape

x1_train.head()

x1_test.head()

